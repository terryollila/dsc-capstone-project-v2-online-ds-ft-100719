{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The film industry worldwide does upwards of 50 billion dollars in box office sales, not counting home entertainment revenue, which brings it up closer to 150 billion dollars. Operating within that brick of cash comes with a tremendous amount of risk, with films from major studios sometimes spending a quarter of a billion dollars or more on a single film. Decisions made at smaller studios are no less important to them, as they might be putting their entire livelihoods on the line in the hopes of a hit. And the value of a movie begins with a script.\n",
    "\n",
    "My premise in creating this project was to ascertain whether a movie's critical rating can be determined to any extent by the text of its screenplay alone. Before attaching a cast and director and crew and all of the other costs associated with creating a cinematic work, having some guidance as to the quality of the script itself can be a benefit in minimizing risk. While an algorithm is no substitute for having a human eye on a screenplay, some level of unbiased machine learning can be leveraged to take a closer look. It is also possible to use this process to vet possible licensing if resources are short and slush piles are large.\n",
    "\n",
    "As there are many factors going into a movie's rating, such as cast, director, editing, music, costuming, set design and so on, it is not necessarily expected that a movies critical rating can be determined solely by the text of its screenplay. However, there is still much value to be had if any measureable predictability can be found. At the completion of my modeling, I was ultimately able to predict scripts from good movies and bad movies, as rated by metacritic.com, about 65% of the time. Given the other factors in rating, I feel that is a significant enough to create value in the model. Among the various models I tried, some had a better true positive rate, and others had a better false positive rate, so there are some choices there depending on what use cases might be found, such as when it might be more advantageous to find a good movie versus avoiding a bad one.\n",
    "\n",
    "Recommendations to a given sudio following would be to use modeling to sort potential screenplays into lists of scripts with higher likelihood of success, using modeling to evaluate scripts in process and step back to consider if it needs more work if the model doesn't like it, and for the screenwriters themselves, to check their scripts against the model and if it comes back with a 'bad' rating, potentially rethink their life choices.\n",
    "\n",
    "For further research, I would like to create a text ingestion field in the dashboard that allows a user to insert a body of text and have a prediction returned evaluating the content as a screenplay and assigning a good or bad designation. I would create predictability functionality that would allow a user to choose a 'good' or 'bad' setting and have automatically generated text returned back in the style of either a good or bad screenplay. And I would go deeper into the neural networds when modeling, especially toward regression. They showed promise when using them in this project, but there was insufficient time to build them out using pre-made embedding layers and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import requests\n",
    "import keras\n",
    "import spacy\n",
    "import time\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from ast import literal_eval\n",
    "from importlib import reload\n",
    "\n",
    "# Importing my own functions file.\n",
    "import functions as fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data for this report will be gathered using web scraping from the following web sites:\n",
    "    \n",
    "- metacritic.com for movie rating information.\n",
    "- rottontomatoes.com for additional movie rating information.\n",
    "- SpringfieldSpringfield.co.uk for gathering the screenplay texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were a few attempts at scraping data before finding versions that worked well for my purposes. What follows are the final attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sraping metacritic.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm taking the most highly rated and most lowly rated films as listed on this site. These extremes will be used for training my classification models to pridict if random movies will be highly rated or lowly rated films."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "goods_titles = []\n",
    "\n",
    "for i in range(0,20):\n",
    "    # There are 10 pages to flip through of 100 movies each.\n",
    "    page = requests.get(\n",
    "        'https://www.metacritic.com/browse/movies/score/metascore/all/filtered?page={}'.format(i),\n",
    "        headers={'User-Agent': 'Chrome/80.0.3987.116'})\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Now that we've gotten the content from the page, we need to loop through each element.\n",
    "    for i in range(0,100,1):\n",
    "        title = soup.find_all('span', class_=\"title numbered\")[i]\\\n",
    "            .next_sibling.next_sibling.contents[1].contents[0]\n",
    "        goods_titles.append(title)\n",
    "    \n",
    "    # We're only pinging 10 times but might as well be safe since it costs like\n",
    "    # nothing.\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that I won't be able to come up with screenplays for every single movie, I'm taking 2000 great and 2000 terrible films, in the hopes of winding up with at least 1000 of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(goods_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "goods_titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terrible Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bads_titles = []\n",
    "\n",
    "for i in range(110, 130):\n",
    "    # There are 10 pages to flip through of 100 movies each.\n",
    "    page = requests.get(\n",
    "        'https://www.metacritic.com/browse/movies/score/metascore/all/filtered?page={}'.format(i),\n",
    "        headers={'User-Agent': 'Chrome/80.0.3987.116'})\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Now that we've gotten the content from the page, we need to loop through each element.\n",
    "    for i in range(0,100,1):\n",
    "        try:\n",
    "            title = soup.find_all('span', class_=\"title numbered\")[i]\\\n",
    "            .next_sibling.next_sibling.contents[1].contents[0]\n",
    "            bads_titles.append(title)\n",
    "        except:\n",
    "            pass\n",
    "    # We're only pinging 10 times but might as well be safe since it costs like\n",
    "    # nothing.\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bads_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goods_formatted = fun.format_titles(goods_titles)\n",
    "bads_formatted = fun.format_titles(bads_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping rottentomatoes.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rottentomatoes.com information will be used for linear regression. Whereas with metacritic we were using only the best and worst for classification, here I'm using samples from the entire spectrum for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_rotten_movies = []\n",
    "rotten_scores = []\n",
    "for i in range(0, 101):\n",
    "    page = requests.get(\"https://www.rottentomatoes.com/browse/\"\n",
    "                        \"dvd-streaming-all?minTomato={}&maxTomato={}&services\"\n",
    "                        \"=amazon;hbo_go;itunes;netflix_iw;vudu;amazon_prime;\"\n",
    "                        \"fandango_now&genres=1;2;4;5;6;8;9;10;11;13;18;14\"\n",
    "                        \"&sortBy=release\".format(i, i+1))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    page = soup.get_text()\n",
    "    comp = re.compile('\"\\/m\\/\\w+\"')\n",
    "    movies = comp.findall(page)\n",
    "    movies_unique = list(set([movie[4:-1] for movie in movies]))\n",
    "    rotten_scores.extend([i for _ in movies_unique])\n",
    "    all_rotten_movies.extend(movies_unique)\n",
    "    print(i)\n",
    "    print(movies_unique)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chopping off the year for those movies that have it.\n",
    "rotten_movies_noyear = [film[:-5] if film[-4:-2] == '20' else film \n",
    "        for film in all_rotten_movies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_form = fun.format_titles(rotten_movies_noyear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping in the Screenplays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this will be an analysis centered around natural language processing, my primary data source will be the screenplay text from every movie.\n",
    "\n",
    "Unfortunately, I was eventually locked out of SpringfieldSpringfield.co.uk, the site where I retrieved the content from, due to too many 'visits'. As I had hit the site upwards of 10k times in the course of a few days, it was probably a fair call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the good screenplays along with a list of titles I couldn't find scripts for.\n",
    "the_good, good_errors = fun.grab_screenplays(goods_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting the good screenplays along with a list of titles I couldn't find scripts for.\n",
    "the_bad, bad_errors = fun.grab_screenplays(bads_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting them both in a DataFrame to be used as data for the rest of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_good = pd.DataFrame([the_good]).T\n",
    "df_bad = pd.DataFrame([the_bad]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now getting the screenplays for the rottentomatoes titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_movies, rotten_errors = fun.grab_screenplays(rotten_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_df = pd.DataFrame(columns=['titles',\n",
    "                                  'titles_formatted',\n",
    "                                  'rotten_scores',\n",
    "                                  'scripts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the titles and scores loaded into the DataFrame.\n",
    "rotten_df.titles = rotten_form\n",
    "rotten_df.RottenScores = rotten_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting things formatted correctly.\n",
    "rotten_df.scripts = rotten_df.titles_formatted.apply(\n",
    "    lambda x: rotten_scripts[0][x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all of this data into csv files to be saved in case of setbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_good.to_csv('../project_resources/df_good_obtain.csv')\n",
    "df_bad.to_csv('../project_resources/df_bad_obtain.csv')\n",
    "rotten_df.to_csv('../project_resources/rotten_df_obtain.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrubbing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading back in data that was created in 'obtaining' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had intended to use files directly from the 'obtain' notebook to be cleaner,\n",
    "# but I can't get back into the screenplay site, so I'll have to improvise.\n",
    "# df_good = pd.read_csv('df_good_obtain.csv')\n",
    "# df_bad = pd.read_csv('df_bad_obtain.csv')\n",
    "# rotten_df = pd.read_csv('rotten_df_obtain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_good = pd.read_csv('../project_resources/df_good.csv', index_col=False)\n",
    "df_bad = pd.read_csv('../project_resources/df_bad.csv', index_col=False)\n",
    "rotten_df = pd.read_csv('../project_resources/rotten_df.csv', index_col=0)\n",
    "\n",
    "rotten_df.columns = ['titles', 'titles_formatted', 'rotten_scores', \n",
    "                     'scripts', 'all_together_now', 'no_stop', 'just_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metacritic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-setting columns and index after re-importing.\n",
    "df_good.columns = ['titles', 'scripts', 'good_or_bad']\n",
    "df_bad.columns = ['titles', 'scripts', 'good_or_bad']\n",
    "\n",
    "# Adding labels, combining good and bad, and dropping missing scripts.\n",
    "df_good['good_or_bad'] = 1\n",
    "df_bad['good_or_bad'] = 0\n",
    "\n",
    "screenplays = pd.concat([df_good, df_bad])\n",
    "screenplays.columns = ['titles', 'scripts', 'good_or_bad']\n",
    "\n",
    "screenplays.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of good screeenplays:  1270\n",
      "# of bad screenplays:  1514\n"
     ]
    }
   ],
   "source": [
    "print('# of good screeenplays: ', len(screenplays[screenplays['good_or_bad'] == 1]))\n",
    "print('# of bad screenplays: ', len(screenplays[screenplays['good_or_bad'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting imported screenplays back to lists from strings\n",
    "screenplays.scripts = screenplays.scripts.apply(literal_eval)\n",
    "\n",
    "# Series with a list of lines for each screenplay.\n",
    "good_to_count = screenplays[screenplays['good_or_bad'] == 1]\n",
    "bad_to_count = screenplays[screenplays['good_or_bad'] == 0]\n",
    "\n",
    "# Single string of all good words.\n",
    "splice_scripts = ''\n",
    "for script in good_to_count['scripts']:\n",
    "    splice_scripts += ''.join(script)\n",
    "\n",
    "all_good_words = ''.join(splice_scripts)\n",
    "\n",
    "# Single string of all bad words.\n",
    "splice_scripts = ''\n",
    "for script in bad_to_count['scripts']:\n",
    "    splice_scripts += ''.join(script)\n",
    "\n",
    "all_bad_words = ''.join(splice_scripts)\n",
    "\n",
    "# Lists of all words lumped together and tokenized\n",
    "good_data = word_tokenize(all_good_words)\n",
    "bad_data = word_tokenize(all_bad_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating some simple word metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good words total:  14020073\n",
      "bad words total:  15477699\n",
      "--------------------------------------------------------------------------------\n",
      "good vocabulary:  172220\n",
      "bad vocabulary:  183840\n",
      "--------------------------------------------------------------------------------\n",
      "good % vocab to total:  0.0123\n",
      "good % vocab to total:  0.0119\n",
      "--------------------------------------------------------------------------------\n",
      "Average Good # Words:  11039.427559055119\n",
      "Average Bad # Words:  10223.050858652576\n",
      "--------------------------------------------------------------------------------\n",
      "Ave difference by words, good vs bad:  0.08\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('good words total: ', len(good_data))\n",
    "print('bad words total: ', len(bad_data))\n",
    "print('----'*20)\n",
    "\n",
    "print('good vocabulary: ', len(set(good_data)))\n",
    "print('bad vocabulary: ', len(set(bad_data)))\n",
    "print('----'*20)\n",
    "\n",
    "print('good % vocab to total: ', round(len(set(good_data)) / len(good_data),4))\n",
    "print('good % vocab to total: ', round(len(set(bad_data)) / len(bad_data),4))\n",
    "print('----'*20)\n",
    "\n",
    "# Total words divided by total number of sripts.\n",
    "print('Average Good # Words: ', len(good_data) / len(good_to_count))\n",
    "print('Average Bad # Words: ', len(bad_data) / len(bad_to_count))\n",
    "print('----'*20)\n",
    "\n",
    "print('Ave difference by words, good vs bad: ', \n",
    "      round(((len(good_data) / len(good_to_count)) \\\n",
    "       - (len(bad_data) / len(bad_to_count))) / (len(bad_data) \\\n",
    "                                                 / len(bad_to_count)),2))\n",
    "print('----'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting punctuation and comparing good to bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good ':' ratio:  0.001625\n",
      "Bad ':' ratio:  0.001428\n",
      "Good-Bad % for ':' 0.14\n",
      "--------------------------------------------------\n",
      "Good ';' ratio:  0.000048\n",
      "Bad ';' ratio:  0.000114\n",
      "Good-Bad % for ';' -0.58\n",
      "--------------------------------------------------\n",
      "Good ',' ratio:  0.047181\n",
      "Bad ',' ratio:  0.048443\n",
      "Good-Bad % for ',' -0.03\n",
      "--------------------------------------------------\n",
      "Good '...' ratio:  0.011329\n",
      "Bad '...' ratio:  0.010441\n",
      "Good-Bad % for '...' 0.09\n",
      "--------------------------------------------------\n",
      "Good '!' ratio:  0.010268\n",
      "Bad '!' ratio:  0.014268\n",
      "Good-Bad % for '!' -0.28\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for p in [':', ';', ',', '...', '!']:\n",
    "    good_p = good_data.count(p) / len(good_data)\n",
    "    bad_p = bad_data.count(p) / len(bad_data)\n",
    "    print(f'Good \\'{p}\\' ratio: ', np.format_float_positional(round(good_p, 6)))\n",
    "    print(f'Bad \\'{p}\\' ratio: ', np.format_float_positional(round(bad_p,6)))\n",
    "    print(f'Good-Bad % for \\'{p}\\'', round((good_p - bad_p) / bad_p,2))\n",
    "    print('-----'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad scripts on average use 28% more exclamation marks and 58% more semicolons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now creating additional columns for scripts in different formats. Right now, each script line is an element of a list. Breaking those apart into one long string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "splice_scripts = ''\n",
    "for script in screenplays['scripts']:\n",
    "    splice_scripts += ''.join(script)\n",
    "\n",
    "all_words = ''.join(splice_scripts)\n",
    "\n",
    "temp = []\n",
    "for script in screenplays['scripts']:\n",
    "    temp.append(''.join(script))\n",
    "\n",
    "# This has each script as one long string inside of its cell, \n",
    "# as opposed with a list of lines.\n",
    "screenplays['all_together_now'] = temp\n",
    "\n",
    "data = word_tokenize(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for # of unique tokens so I know roughly how many to play with when modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278462"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function will be used in an apply function remove the stop words for purposes further down. One will keep punctuation and one will remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b65161ba6fb4ffdb3872beec2fcc306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2784), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86f4bfe48d145b0ba1e6e13cc18cb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2784), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "screenplays['no_stop'] = screenplays['all_together_now']\\\n",
    "    .progress_apply(fun.stop_it, punct=False)\n",
    "\n",
    "screenplays['just_words'] = screenplays['all_together_now']\\\n",
    "    .progress_apply(fun.stop_it, punct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rottentomatoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for script in rotten_df.scripts:\n",
    "    temp.append(''.join(script))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has each script as one long string inside of its cell, \n",
    "# as opposed with a list of lines.\n",
    "rotten_df['all_together_now'] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function again for rotten_df this time to remove stop words for one columns of scripts and remove stop words plus punctuation for another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455c8fe54a5941bd94edd4a0fcbfec74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1536), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634f982aa5834d929c25659a102e12e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1536), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rotten_df['no_stop'] = rotten_df['all_together_now']\\\n",
    "    .progress_apply(fun.stop_it, punct=False)\n",
    "\n",
    "rotten_df['just_words'] = rotten_df['all_together_now']\\\n",
    "    .progress_apply(fun.stop_it, punct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to rename the period since it causes problems later on.\n",
    "rotten_df.rename(columns={'.':'PER'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotten_df.dropna(inplace=True)\n",
    "# screenplays.dropna(inplace=True)\n",
    "\n",
    "rotten_df = rotten_df.drop_duplicates(subset=['titles']).copy()\n",
    "screenplays = screenplays.drop_duplicates(['titles']).copy()\n",
    "\n",
    "screenplays.to_csv('../project_resources/screenplays_scrub.csv')\n",
    "rotten_df.to_csv('../project_resources/rotten_df_scrub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting punctuation and parts of speech\n",
    "def count_punct(text, punk):\n",
    "    return text.count(punk) / len(text.split())\n",
    "\n",
    "def count_pos(x):\n",
    "    return x.count_by(spacy.attrs.POS)\n",
    "\n",
    "def count_tag(x):\n",
    "    return x.count_by(spacy.attrs.TAG)\n",
    "\n",
    "# Determining the average sentence length for a given script.\n",
    "def sent_len(str):\n",
    "    doc = nlp(str)\n",
    "    count = 0\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        count += 1\n",
    "    \n",
    "    return len(str.split()) / count\n",
    "\n",
    "# These two find the acutal POS name for the code number supplied, for\n",
    "# creating the column names.\n",
    "def POS_reverse_lookup_n_ratio(x, code):\n",
    "    try:\n",
    "        return x.POS_counts[code] / x.word_count\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def TAG_reverse_lookup_n_ratio(x, code):\n",
    "    try:\n",
    "        return x.TAG_counts[code] / x.word_count\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading back in data we scubbed in 'scrubbing'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenplays = pd.read_csv('../project_resources/screenplays_scrub.csv', index_col=0)\n",
    "rotten_df = pd.read_csv('../project_resources/rotten_df_scrub.csv', index_col=0)\n",
    "\n",
    "screenplays.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding column showing if a title is above or below a score of 50.\n",
    "rotten_df['good_or_bad'] = rotten_df.rotten_scores.apply(\n",
    "    lambda x: 1 if x >=50 else 0)\n",
    "\n",
    "# Re-ordering columns to line up the two data sources for later use.\n",
    "screen_cols = list(screenplays.columns)\n",
    "screen_cols.remove('good_or_bad')\n",
    "screen_cols.append('good_or_bad')\n",
    "screenplays = screenplays[screen_cols].copy()\n",
    "\n",
    "rotten_cols = list(rotten_df.columns)\n",
    "rotten_cols.remove('rotten_scores')\n",
    "rotten_cols.append('rotten_scores')\n",
    "rotten_df = rotten_df[rotten_cols].copy()\n",
    "rotten_df.drop('titles', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotten_df = rotten_df[sort_cols]\n",
    "rotten_df.rename(columns={'titles_formatted':'titles'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Derichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the categories inherent in the screenplays by analyzing the words and grouping the screenplays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([rotten_df[['titles', 'just_words']],\n",
    "                         screenplays[['titles', 'just_words']]],\n",
    "                         ignore_index=True)\n",
    "\n",
    "combined_df.drop_duplicates(subset='titles', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=.1, max_df=.6)\n",
    "victor = cv.fit_transform(combined_df.just_words)\n",
    "LDA = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "combined_df['category'] = LDA.fit_transform(victor).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the category word groupings, then creating materiels to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR CATEGORY #0\n",
      "['government', 'fight', 'human', 'war', 'law', 'body', 'children', 'power', 'court', 'country', 'state', 'president', 'death', 'police', 'dr']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR CATEGORY #1\n",
      "['buddy', 'wow', 'team', 'cool', 'uh', 'ball', 'aint', 'ya', 'ah', 'em', 'dog', 'ha', 'game', 'whoa', 'ok']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR CATEGORY #2\n",
      "['air', 'safe', 'jim', 'ship', 'war', 'jack', 'officer', 'clear', 'shot', 'shoot', 'fire', 'police', 'john', 'captain', 'gun']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR CATEGORY #3\n",
      "['ooh', 'whoa', 'ah', 'christmas', 'cool', 'wow', 'bye', 'daddy', 'charlie', 'sam', 'honey', 'jack', 'um', 'uh', 'mom']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR CATEGORY #4\n",
      "['uh', 'grunts', 'radio', 'crowd', 'chatter', 'cheering', 'speaking', 'indistinct', 'continues', 'laughs', 'playing', 'laughing', 'sighs', 'chuckles', 'music']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR CATEGORY #5\n",
      "['aint', 'poor', 'sister', 'darling', 'war', 'brother', 'child', 'death', 'lady', 'children', 'king', 'lord', 'dear', 'shall', 'mrs']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR CATEGORY #6\n",
      "['gun', 'brother', 'goddamn', 'motherfucker', 'dude', 'fucked', 'em', 'yo', 'jesus', 'bitch', 'ass', 'fuckin', 'aint', 'fucking', 'fuck']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR CATEGORY #7\n",
      "['buy', 'women', 'parents', 'york', 'bye', 'perfect', 'dinner', 'book', 'party', 'honey', 'movie', 'girls', 'sex', 'married', 'mom']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR CATEGORY #8\n",
      "['white', 'women', 'ii', 'sort', 'mmm', 'mm', 'wow', 'alright', 'mmhmm', 'john', 'bob', 'hmm', 'ah', 'um', 'uh']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR CATEGORY #9\n",
      "['groaning', 'breathing', 'chuckles', 'continues', 'music', 'groans', 'screams', 'panting', 'mary', 'sighs', 'michael', 'grunting', 'grunts', 'gasps', 'screaming']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This will house the dictionaries for using in the dashboard.\n",
    "cat_word_dicts = []\n",
    "\n",
    "# Priting out the word gropuings and filling in the cat_word_dicts list.\n",
    "for index,topic in enumerate(LDA.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR CATEGORY #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')\n",
    "    cat_word_dicts.append({index:[cv.get_feature_names()[i]\\\n",
    "                                  for i in topic.argsort()[-15:]]})\n",
    "#     cat_word_dicts.append([cv.get_feature_names()[i]\\\n",
    "#                           for i in topic.argsort()[-15:]])\n",
    "\n",
    "cat_word_df = pd.DataFrame()\n",
    "\n",
    "# Creating a dataframe for export to .csv for later use.\n",
    "for i in range(len(cat_word_dicts)):\n",
    "    cat_word_df[i] = cat_word_dicts[i][i]\n",
    "\n",
    "# Creating category mapping for all dataframes to indicate how I see \n",
    "# The groupings. This part is just based on my looking them over.\n",
    "category_map = {0: 'Dark & Political', 1: 'Sports, Comedy, Silly Horror', \n",
    "                2: 'Conflict', 3: 'Holiday, Films I Haven\\'t Seen',\n",
    "                4: 'Light-Hearted', 5: 'Unusual Language or Slang', \n",
    "                6: 'Violence & Gangster', 7: 'Romance & Light Drama', \n",
    "                8: 'Life Stories', 9: 'Straight Up Horror'}\n",
    "combined_df['category_label'] = combined_df.category.map(category_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to need the combined dataframes with LDA categories for the dashboard later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenplays = pd.merge(screenplays, combined_df[['titles','category', 'category_label']],\n",
    "                how='left', on='titles')\n",
    "\n",
    "rotten_df = pd.merge(rotten_df, combined_df[['titles','category', 'category_label']],\n",
    "                how='left', on='titles')\n",
    "\n",
    "cat_word_df.to_csv('../project_resources/cat_word_df.csv', header=True)\n",
    "\n",
    "combined_df.to_csv('../project_resources/combined_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metacritic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be creating quite a few new columns filled with script attributes such as word counts, sentiment, and parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting in some counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenplays['word_count'] = screenplays.all_together_now.apply(\n",
    "    lambda x: len(x.split()))\n",
    "screenplays['unique_words'] = screenplays.no_stop.apply(\n",
    "    lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating columns for sentiment scoring: positive, negative, neutral, and compound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc94eaac79042c68405133e74b7d536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2765), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "screenplays['sentiment_scores'] = screenplays.no_stop.progress_apply(\n",
    "    lambda x: sid.polarity_scores(x))\n",
    "\n",
    "screenplays['sentiment_negative'] = screenplays.sentiment_scores.apply(\n",
    "    lambda x: x['neg'])\n",
    "screenplays['sentiment_neutral'] = screenplays.sentiment_scores.apply(\n",
    "    lambda x: x['neu'])\n",
    "screenplays['sentiment_positive'] = screenplays.sentiment_scores.apply(\n",
    "    lambda x: x['pos'])\n",
    "screenplays['sentiment_compound'] = screenplays.sentiment_scores.apply(\n",
    "    lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing anything with fewer words words so that short scripts don't throw off the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenplays_cut = screenplays.copy().drop(\n",
    "    index=(screenplays[screenplays['word_count'] < 1000].index), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More columns for punctuation ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenplays_cut['colon_ratios'] = screenplays_cut.no_stop.apply(\n",
    "    lambda x: count_punct(x, ':'))\n",
    "screenplays_cut['semi_ratios'] = screenplays_cut.no_stop.apply(\n",
    "    lambda x: count_punct(x, ';'))\n",
    "screenplays_cut['comma_ratios'] = screenplays_cut.no_stop.apply(\n",
    "    lambda x: count_punct(x, ','))\n",
    "screenplays_cut['ellipsis_ratios'] = screenplays_cut.no_stop.apply(\n",
    "    lambda x: count_punct(x, '...'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2502cdcf4e4b8d9073a89acdde7247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2745), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "screenplays_cut['sentence_length'] = screenplays_cut.no_stop.progress_apply(\n",
    "    sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking out both coarse and fine grained approaches to parts of speech, to create a ratio of how many of each is used in each screenplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e02f4e83c304e53938bda9ef85f35ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2745), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "screenplays_cut['nlp'] = screenplays_cut.all_together_now.progress_apply(nlp)\n",
    "screenplays_cut.to_csv('../project_resources/screenplays_cut.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5b19e64b034f0cbde99667292e0041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2745), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc051282821453e8f8ceffdec570c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2745), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Applying the 'progress_apply' function to each row to get the actual words.\n",
    "screenplays_cut['POS_counts'] = screenplays_cut.nlp.progress_apply(count_pos)\n",
    "screenplays_cut['TAG_counts'] = screenplays_cut.nlp.progress_apply(count_tag)\n",
    "\n",
    "# POS_codes and TAG codes will be used to identify unique codes \n",
    "# used throughout vocabulary.\n",
    "POS_codes = set()\n",
    "\n",
    "for i, item in enumerate(screenplays_cut.POS_counts):\n",
    "    POS_codes.update(item.keys())\n",
    "\n",
    "TAG_codes = set()\n",
    "\n",
    "for i, item in enumerate(screenplays_cut.TAG_counts):\n",
    "    TAG_codes.update(item.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scripts = set()\n",
    "\n",
    "for i, script in enumerate(screenplays_cut.just_words):\n",
    "    all_scripts.update(script.split())\n",
    "\n",
    "all_scripts = [script for script in all_scripts]\n",
    "\n",
    "all_scripts = ' '.join(all_scripts)\n",
    "\n",
    "all_scripts = nlp(all_scripts[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding POS columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAG_lookup will be used for adding POS word to column name.\n",
    "TAG_lookup = {}\n",
    "for code in TAG_codes:\n",
    "    key = code\n",
    "    value = all_scripts.vocab[code].text\n",
    "    TAG_lookup[key] = value\n",
    "\n",
    "POS_lookup = {}\n",
    "for code in POS_codes:\n",
    "    key = code\n",
    "    value = all_scripts.vocab[code].text\n",
    "    POS_lookup[key] = value\n",
    "\n",
    "# Adding POS column and adding POS name as column name.\n",
    "for code, abb in POS_lookup.items():\n",
    "    screenplays_cut[abb] = screenplays_cut.apply(\n",
    "        POS_reverse_lookup_n_ratio, args=[code], axis=1)\n",
    "\n",
    "# Adding TAG column and adding TAG name as column name.\n",
    "for code, abb in TAG_lookup.items():\n",
    "    screenplays_cut[abb] = screenplays_cut.apply(\n",
    "        TAG_reverse_lookup_n_ratio, args=[code], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenplays_cut = screenplays_cut.fillna(0)\n",
    "\n",
    "screenplays_cut.to_csv('../project_resources/screenplays_cut.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rottentomatoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with metacritic, will be creating quite a few new columns filled with script attributes such as word counts, sentiment, and parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting in some counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_df['word_count'] = rotten_df.all_together_now.apply(\n",
    "    lambda x: len(x.split()))\n",
    "rotten_df['unique_words'] = rotten_df.no_stop.apply(\n",
    "    lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_df_cut = rotten_df.copy().drop(\n",
    "    index=(rotten_df[rotten_df['word_count'] < 1000].index), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating columns for sentiment scoring: positive, negative, neutral, and compound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dc057b97f34b72b0097ccc7692021c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1536), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "rotten_df_cut['sentiment_scores'] = rotten_df.no_stop.progress_apply(\n",
    "    lambda x: sid.polarity_scores(x))\n",
    "\n",
    "rotten_df_cut['sentiment_negative'] = rotten_df_cut.sentiment_scores.apply(\n",
    "    lambda x: x['neg'])\n",
    "rotten_df_cut['sentiment_neutral'] = rotten_df_cut.sentiment_scores.apply(\n",
    "    lambda x: x['neu'])\n",
    "rotten_df_cut['sentiment_positive'] = rotten_df_cut.sentiment_scores.apply(\n",
    "    lambda x: x['pos'])\n",
    "rotten_df_cut['sentiment_compound'] = rotten_df_cut.sentiment_scores.apply(\n",
    "    lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing anything with fewer words words so that short scripts don't throw off the modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More columns for punctuation ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_df_cut['colon_ratios'] = rotten_df_cut.no_stop.apply(\n",
    "    lambda x: count_punct(x, ':'))\n",
    "rotten_df_cut['semi_ratios'] = rotten_df_cut.no_stop.apply(\n",
    "    lambda x: count_punct(x, ';'))\n",
    "rotten_df_cut['comma_ratios'] = rotten_df_cut.no_stop.apply(\n",
    "    lambda x: count_punct(x, ','))\n",
    "rotten_df_cut['ellipsis_ratios'] = rotten_df_cut.no_stop.apply(\n",
    "    lambda x: count_punct(x, '...'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_df_cut['sentence_length'] = rotten_df_cut.no_stop.progress_apply(\n",
    "    sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking out both coarse and fine grained approaches to parts of speech, to create a ratio of how many of each is used in each screenplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_df_cut['nlp'] = rotten_df_cut.all_together_now.progress_apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_df_cut.to_csv('../project_resources/rotten_df_cut.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotten_df_cut = pd.read_csv('../project_resources/rotten_df_cut.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the 'progress_apply' function to each row to get the actual words.\n",
    "rotten_df_cut['POS_counts'] = rotten_df_cut.nlp.progress_apply(count_pos)\n",
    "\n",
    "rotten_df_cut['TAG_counts'] = rotten_df_cut.nlp.progress_apply(count_tag)\n",
    "\n",
    "# POS_codes and TAG codes will be used to identify unique codes \n",
    "# used throughout vocabulary.\n",
    "POS_codes = set()\n",
    "\n",
    "for i, item in enumerate(rotten_df_cut.POS_counts):\n",
    "    POS_codes.update(item.keys())\n",
    "\n",
    "TAG_codes = set()\n",
    "\n",
    "for i, item in enumerate(rotten_df_cut.TAG_counts):\n",
    "    TAG_codes.update(item.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding POS columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAG_lookup will be used for adding POS word to column name.\n",
    "TAG_lookup = {}\n",
    "for code in TAG_codes:\n",
    "    key = code\n",
    "    value = all_scripts.vocab[code].text\n",
    "    TAG_lookup[key] = value\n",
    "    \n",
    "POS_lookup = {}\n",
    "for code in POS_codes:\n",
    "    key = code\n",
    "    value = all_scripts.vocab[code].text\n",
    "    POS_lookup[key] = value\n",
    "\n",
    "# Adding POS column and adding POS name as column name.\n",
    "for code, abb in POS_lookup.items():\n",
    "    rotten_df_cut[abb] = rotten_df_cut.apply(\n",
    "        POS_reverse_lookup_n_ratio, args=[code], axis=1)\n",
    "\n",
    "# Adding TAG column and adding TAG name as column name.\n",
    "for code, abb in TAG_lookup.items():\n",
    "    rotten_df_cut[abb] = rotten_df_cut.apply(\n",
    "        TAG_reverse_lookup_n_ratio, args=[code], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_df_cut = rotten_df_cut.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_df_cut.to_csv('../project_resources/rotten_df_cut.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the original eda.ipynb file, I have an extensive set of scatter matrices, histograms, and correlation matrices at this point in the process. However, they took up too much memory to be efficiently placed in this document, but they can be referred to in eda.ipynb as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data previously saved in .csv format from eda.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_df_cut = pd.read_csv('../project_resources/rotten_df_cut.csv', index_col=0)\n",
    "screenplays_cut = pd.read_csv('../project_resources/screenplays_cut.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling columns  for ease of use when modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_cols = list(screenplays_cut.columns)\n",
    "screen_cols.remove('good_or_bad')\n",
    "screen_cols.append('good_or_bad')\n",
    "screenplays_model = screenplays_cut[screen_cols].copy()\n",
    "\n",
    "rotten_cols = list(rotten_df_cut.columns)\n",
    "rotten_cols.remove('good_or_bad')\n",
    "rotten_cols.append('good_or_bad')\n",
    "rotten_cols.remove('rotten_scores')\n",
    "rotten_cols.append('rotten_scores')\n",
    "rotten_model = rotten_df_cut[rotten_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_cols = list(rotten_model.columns[:38])\n",
    "# sort_cols.extend(sorted(list(rotten_model.columns[38:-2])))\n",
    "# sort_cols.extend(list(rotten_model.columns[-2:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first set of models will employ TFIDF Vectorization using a variety of classifiers.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the modeling, I'm going to use the extreme ratings I got from metacritic.com for training the models, then use the rottentomatoes data, which is uniformly distributed from bad movies to good movies, to test the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = screenplays_cut.no_stop\n",
    "X_test = rotten_df_cut.no_stop\n",
    "\n",
    "y_train = screenplays_cut.good_or_bad\n",
    "y_test = rotten_df_cut.good_or_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun.hybrid_classifiers(X_train, X_test, y_train, y_test, LinearSVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun.hybrid_classifiers(X_train, X_test, y_train, y_test, SVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun.hybrid_classifiers(X_train, X_test, y_train, y_test, \n",
    "                   XGBClassifier(max_depth=8,\n",
    "                                    criterion='entropy',\n",
    "                                    min_samples_split=14,\n",
    "                                    min_samples_leaf=1,\n",
    "                                    max_features=160))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the neural network works so differently from the other classifiers, I haven't used a function, and the main code is all here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting to put the data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "\n",
    "# Undersampling to get things even just for ease of use.\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(pd.DataFrame(X_train), y_train)\n",
    "\n",
    "X_resampled = X_resampled.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vector from the full batch of scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000, max_df=.95, min_df=.1, \n",
    "                        ngram_range=(1,2))\n",
    "X_t_train = tfidf.fit_transform(X_resampled)\n",
    "X_t_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data into the format it will need to be in for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t_train = pd.SparseDataFrame(X_t_train, columns=tfidf.get_feature_names(),\n",
    "                           default_fill_value=0)\n",
    "X_t_test = pd.SparseDataFrame(X_t_test, columns=tfidf.get_feature_names(),\n",
    "                           default_fill_value=0)\n",
    "\n",
    "X_t_num = np.array(X_t_train)\n",
    "X_t_test_num = np.array(X_t_test)\n",
    "\n",
    "y_t_num = np.array(y_resampled)\n",
    "y_t_test_num = np.array(y_test)\n",
    "\n",
    "layer_input = X_t_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I played manually with a lot of different layers, neurons, drop layers, and regularization. Simple seemed to work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(layer_input, input_dim=layer_input, activation='relu'))\n",
    "model.add(Dense(50, input_dim=layer_input, activation='relu'))\n",
    "model.add(Dense(50, input_dim=50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_t_num, y_t_num, epochs=4, batch_size=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_t_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_t_test_num, y_t_test_num, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_t_test_num)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(confusion, figsize=(7,7), colorbar=True,\n",
    "                  show_normed=True, cmap=plt.cm.Reds);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, a pretty positive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models here were strong, overall. Most of them had accuracy of around 65%, with poor true positive rate of around 50, which means they were quite good at predicting which scripts were bad but not great at predicting which scripts were good. The decision tree had good true positive rate at 61%, but the accuracy wasn't as good at 58%. XG Boost had the best accuracy at 66, but the true positive rate of precisely 50%, with true negative rate 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this set of modeling, I'm using only the script attributes from the dataframe, so basically word counts, punctuation ratios, sentence length, sentiment, and parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_dummies = pd.get_dummies(screenplays_model['category'])\n",
    "rotten_dummies = pd.get_dummies(rotten_model['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(screenplays_model.columns[7:9])\n",
    "columns.extend(list(screenplays_model.columns[10:19]))\n",
    "start = list(screenplays_model.columns).index('PROPN')\n",
    "columns.extend(list(screenplays_model.columns[start:-1]))\n",
    "    \n",
    "X = screenplays_model[columns]\n",
    "X_train = pd.merge(screenplays_model[columns], screen_dummies, left_index=True, \n",
    "             right_index=True)\n",
    "X_test = pd.merge(rotten_model[columns], rotten_dummies, left_index=True, \n",
    "             right_index=True)\n",
    "\n",
    "y_train = screenplays_model.good_or_bad\n",
    "y_test = rotten_model.good_or_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun.hybrid_classifiers(X_train, X_test, y_train, y_test, \n",
    "                   classifier=XGBClassifier(),use_tfidf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fun.hybrid_classifiers(X_train, X_test, y_train, y_test, \n",
    "                   classifier=RandomForestClassifier(random_state=42,\n",
    "                                                    max_depth=8,\n",
    "                                                    criterion='entropy',\n",
    "                                                    min_samples_split=14,\n",
    "                                                    min_samples_leaf=1,\n",
    "#                                                     max_features=10),\n",
    "                                                    ),\n",
    "                  use_tfidf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripts Attributes Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set was much lower than the TFIDF, with scores that were only a little above random chance. The neural network simply classified everything as positive. I'm sure I could have done something to improve this batch given time, but for now, as it was so much lower than the TFIDF set, I chose to let it go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined TDIF & Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this more experimental bunch, I thought I would try merging the TFIDF vecctorization matrix with the attributes from the original dataframe, wondering if more information might be better, and how the engineered features might play against the word matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_dummies = pd.get_dummies(screenplays_model['category'])\n",
    "rotten_dummies = pd.get_dummies(rotten_model['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(screenplays_model.columns[7:9])\n",
    "columns.extend(list(screenplays_model.columns[10:19]))\n",
    "start = list(screenplays_model.columns).index('PROPN')\n",
    "columns.extend(list(screenplays_model.columns[start:-1]))\n",
    "\n",
    "X_train = screenplays_model[columns].copy()\n",
    "X_test = rotten_model[columns].copy()\n",
    "\n",
    "y_train = screenplays_model.good_or_bad.copy()\n",
    "y_test = rotten_model.good_or_bad.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train = screenplays_model.no_stop\n",
    "X2_test = rotten_model.no_stop\n",
    "y_train = screenplays_model.good_or_bad\n",
    "y_test = rotten_model.good_or_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = pd.DataFrame(X2_train, columns=['temp'])\n",
    "# temp[temp['temp'].isna() == True].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# screenplays_model.iloc[[40, 69, 101, 106, 147, 175, 264, 303, 343, 371, 392,\n",
    "#              464, 656, 811, 963, 1099, 2024, 2044, 2066, 2265, 2554, 2600]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fun.hybrid_classifier_combo(X_train, X_test, X2_train, X2_test,\n",
    "                        y_train, y_test, LinearSVC(C=.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun.hybrid_classifier_combo(X_train, X_test, X2_train, X2_test,\n",
    "                        y_train, y_test, LogisticRegression(C=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fun.hybrid_classifier_combo(X_train, X_test, X2_train, X2_test,\n",
    "                        y_train, y_test, \n",
    "                        RandomForestClassifier(random_state=42, \n",
    "                                               n_jobs=-1),\n",
    "                        feature_importance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of my top models from the TFIDF modeling section. I'll use it as the first phase of a two-part modeling scenario.\n",
    "\n",
    "First, I'm going to send in the movies from metacritic to train the first model, using the rottentomatoes data for testing. Then I'll marry the test predictions to the rottentomatoes (test) data, and send it through the second model, where it will get split into a proper train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = screenplays_cut.no_stop\n",
    "X_test = rotten_df_cut.no_stop\n",
    "\n",
    "y_train = screenplays_cut.good_or_bad\n",
    "y_test = rotten_df_cut.good_or_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_predictions = fun.hybrid_classifiers(X_train, X_test, y_train, y_test, SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = screenplays_model.no_stop\n",
    "# y = screenplays_model.good_or_bad\n",
    "\n",
    "# model_1_predictions = fun.stacked_classifier(X, y, SVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I'm getting the dummies and putting them all in one dataframe to be joined later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_dummies = pd.get_dummies(screenplays_model['category'])\n",
    "rotten_dummies = pd.get_dummies(rotten_model['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = ['sentiment_neutral', 'sentence_length', 'PRON', 'CCONJ', 'PUNCT', 'NNS',\n",
    "#  '_SP', 'VBD', 'WDT', 'VB', 'PRP', 'RP', 'PRP$', 'CC', '.', 'IN', '-RRB-',\n",
    "#  'VBP', 'WP', 'HYPH']\n",
    "\n",
    "columns = ['word_count', 'unique_words']\n",
    "columns.extend(rotten_model.columns[10:19])\n",
    "columns.extend(rotten_model.columns[23:-2])\n",
    "\n",
    "X = pd.merge(rotten_model[columns].copy(), rotten_dummies, left_index=True, \n",
    "             right_index=True)\n",
    "\n",
    "y = rotten_model.good_or_bad.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['predictions'] = model_1_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the test data from previously, since it had not yet seen the overfit training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun.script_classifiers(X, y,\n",
    "                   classifier=RandomForestClassifier(random_state=42,\n",
    "                                                    max_depth=9,\n",
    "                                                    criterion='entropy',\n",
    "                                                    min_samples_split=14,\n",
    "                                                    min_samples_leaf=1,\n",
    "                                                    n_estimators=103),\n",
    "                      use_tfidf=False, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This turned out to be my best model: SVC using TFIDF, then take the predictions from that, add them as a feature (to test data that has not been modeled) and run that data again through a random forest classifier: 68% accuracy, with a true positive rate of 64% and a true negative rate of 72%. Besides having the greatest accuracy, this model also turned out to be the most balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Rottentomatoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I attempted some regression models using data from rottontomatoes.\n",
    "\n",
    "The models here are only the ones I kept. The others had results that were basically no better than chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF with XGBoost Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I grid searched the hell out of this one, only to find that none of it mattered. So I removed the grid search function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(rotten_model.no_stop,\n",
    "                                                    rotten_model.rotten_scores,\n",
    "                                                    test_size=.3,\n",
    "                                                    random_state=42)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000, max_df=.9, min_df=.1, \n",
    "                        ngram_range=(1,2))\n",
    "word_predictors = tfidf.fit_transform(X_train)\n",
    "word_test = tfidf.transform(X_test)\n",
    "\n",
    "model = XGBRegressor(random_state=42, n_estimators=100, \n",
    "                     objective='reg:squarederror')\n",
    "model.fit(word_predictors, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r^2 and mse for train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_scores(X_train, X_test, y_train_y_test, model):\n",
    "    y_hat_train = model.predict(word_predictors)\n",
    "    y_hat_test = model.predict(word_test)\n",
    "    print('train MSE score: ', mse(y_train, y_hat_train))\n",
    "    print('train r2_score: ', r2_score(y_train, y_hat_train))\n",
    "    print('Test MSE score:', mse(y_test, y_hat_test))\n",
    "    print('Test R-sq score:', r2_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = model.predict(word_predictors)\n",
    "y_hat_test = model.predict(word_test)\n",
    "print('train MSE score: ', mse(y_train, y_hat_train))\n",
    "print('train r2_score: ', r2_score(y_train, y_hat_train))\n",
    "print('Test MSE score:', mse(y_test, y_hat_test))\n",
    "print('Test R-sq score:', r2_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_residuals = y_hat_train - list(y_train)\n",
    "test_residuals = y_hat_test - list(y_test)\n",
    "print('Ave deviation from actual: ', round(sum(abs(test_residuals)) / len(test_residuals),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataframe comparing predicted scores to actual scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_v_predicted = X_test.to_frame()\n",
    "\n",
    "pred_scores = list(y_hat_test)\n",
    "\n",
    "actual_v_predicted['predicted_scores'] = pred_scores\n",
    "\n",
    "to_merge = rotten_model[['titles', 'rotten_scores']]\n",
    "\n",
    "actual_v_predicted = actual_v_predicted.merge(to_merge, left_index=True,\n",
    "                                              right_index=True)\n",
    "\n",
    "actual_v_predicted = actual_v_predicted[['titles','rotten_scores',\n",
    "                                         'predicted_scores']]\n",
    "\n",
    "actual_v_predicted['predicted_scores'] = actual_v_predicted.predicted_scores.\\\n",
    "    apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_v_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upside = len(actual_v_predicted[(actual_v_predicted.rotten_scores > 50) \\\n",
    "                   & (actual_v_predicted.predicted_scores > 50)])\n",
    "upside\n",
    "\n",
    "downside = len(actual_v_predicted[(actual_v_predicted.rotten_scores < 50) \\\n",
    "                   & (actual_v_predicted.predicted_scores < 50)])\n",
    "downside\n",
    "\n",
    "all_sides = len(actual_v_predicted)\n",
    "all_sides\n",
    "\n",
    "capture_in_half = (upside + downside) / len(actual_v_predicted)\n",
    "print('Scores captured in same half (upper vs lower): ', capture_in_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_v_predicted.rotten_scores.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_v_predicted.predicted_scores.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(test_residuals, dist=stats.norm, line='45', fit=True)\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_v_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Only with XG Boost Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up all the features for a features-only (non-tfidf) regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to change this as the model chokes on the period.\n",
    "rotten_model.rename(columns={'.':'PER'}, inplace=True)\n",
    "\n",
    "# Dropping out the unnecessary columns.\n",
    "to_drop = list(rotten_model.columns[:7])\n",
    "to_drop.append(rotten_model.columns[9])\n",
    "to_drop.extend(rotten_model.columns[19:22])\n",
    "to_drop.extend(rotten_model.columns[-2:])\n",
    "\n",
    "to_drop_2 = ['sentiment_compound', 'unique_words', 'X', 'JJ', 'NNP', 'DET',\n",
    "             'semi_ratios', 'ADJ']\n",
    "\n",
    "predictors_all = rotten_df_cut.copy()\n",
    "predictors_all.drop(to_drop, axis=1, inplace=True)\n",
    "predictors_all.drop(to_drop_2, axis=1,inplace=True)\n",
    "\n",
    "# Adding dummy categories.\n",
    "# rotten_dummies = pd.get_dummies(rotten_model['category'])\n",
    "# predictors_all = pd.merge(predictors_all, rotten_dummies, left_index=True, \n",
    "#              right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(predictors_all,\n",
    "                                                    rotten_model.rotten_scores,\n",
    "                                                    test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_X_train = pd.DataFrame(scaler.fit_transform(X_train), \n",
    "                          columns = X_train.columns)\n",
    "scaled_X_test = pd.DataFrame(scaler.transform(X_test), \n",
    "                            columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_y_train = scaler.fit_transform(np.array(y_train).reshape(-1,1))\n",
    "scaled_y_test = scaler.transform(np.array(y_test).reshape(-1,1))\n",
    "scaled_y_train = pd.Series(scaled_y_train.reshape(-1,), name='rotten_scores')\n",
    "scaled_y_test = pd.Series(scaled_y_test.reshape(-1,), name='rotten_scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train.fillna(0, inplace=True)\n",
    "scaled_X_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x):\n",
    "    if x == 0:\n",
    "        return math.log((X + 1) / 100)\n",
    "    else:\n",
    "        return math.log(x / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(random_state=42, n_estimators=100, \n",
    "                     objective='reg:squarederror')\n",
    "model.fit(scaled_X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions and calculating residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = model.predict(pd.DataFrame(scaled_X_train))\n",
    "y_hat_test = model.predict(pd.DataFrame(scaled_X_test))\n",
    "\n",
    "train_residuals = y_hat_train - list(y_train)\n",
    "test_residuals = y_hat_test - list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ave deviation from actual: ', round(sum(abs(test_residuals)) / len(test_residuals),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\n",
    "mse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\n",
    "print('Train Mean Squarred Error:', mse_train)\n",
    "print('Test Mean Squarred Error:', mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R^2 score:', r2_score(y_test,y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataframe of predicted scores vs. actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_v_predicted = X_test.copy()\n",
    "\n",
    "pred_scores = list(y_pred)\n",
    "\n",
    "actual_v_predicted['predicted_scores'] = pred_scores\n",
    "\n",
    "to_merge = rotten_model[['titles', 'rotten_scores']]\n",
    "\n",
    "actual_v_predicted = actual_v_predicted.merge(to_merge, left_index=True,\n",
    "                                              right_index=True)\n",
    "\n",
    "actual_v_predicted = actual_v_predicted[['titles','rotten_scores',\n",
    "                                         'predicted_scores']]\n",
    "\n",
    "actual_v_predicted['predicted_scores'] = actual_v_predicted.predicted_scores.\\\n",
    "    apply(lambda x: int(x))\n",
    "\n",
    "actual_v_predicted = actual_v_predicted[actual_v_predicted['predicted_scores'] <= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_v_predicted.predicted_scores.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual_v_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(test_residuals, dist=stats.norm, line='45', fit=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF with Features with XG Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = predictors_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000, max_df=.9, min_df=.1, \n",
    "                        ngram_range=(1,2))\n",
    "X2 = tfidf.fit_transform(rotten_model.no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sparse DataFrame to house both the features and the \n",
    "# processed text.\n",
    "X_temp = pd.SparseDataFrame(X2, columns=tfidf.get_feature_names(),\n",
    "                           default_fill_value=0)\n",
    "# Necessary for next step.\n",
    "X = X.reset_index(drop=True)\n",
    "\n",
    "# Combining text matrix with script attributes.\n",
    "for column in X:\n",
    "    X_temp[column] = X[column]\n",
    "X = X_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(X), y, random_state=42,\n",
    "                                                    test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_X_train = pd.DataFrame(scaler.fit_transform(X_train), \n",
    "                          columns = X_train.columns)\n",
    "scaled_X_test = pd.DataFrame(scaler.transform(X_test), \n",
    "                            columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_y_train = scaler.fit_transform(np.array(y_train).reshape(-1,1))\n",
    "scaled_y_test = scaler.transform(np.array(y_test).reshape(-1,1))\n",
    "scaled_y_train = pd.Series(scaled_y_train.reshape(-1,), name='rotten_scores')\n",
    "scaled_y_test = pd.Series(scaled_y_test.reshape(-1,), name='rotten_scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train.fillna(0, inplace=True)\n",
    "scaled_X_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x):\n",
    "    if x == 0:\n",
    "        return math.log((X + 1) / 100)\n",
    "    else:\n",
    "        return math.log(x / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(random_state=42, n_estimators=100, \n",
    "                     objective='reg:squarederror')\n",
    "model.fit(scaled_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = model.predict(pd.DataFrame(scaled_X_train))\n",
    "y_hat_test = model.predict(pd.DataFrame(scaled_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_residuals = y_hat_train - list(y_train)\n",
    "test_residuals = y_hat_test - list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\n",
    "mse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\n",
    "print('Train Mean Squarred Error:', mse_train)\n",
    "print('Test Mean Squarred Error:', mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('r^2 score:', r2_score(y_test,y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the regression didn't work out that well. I probably could have done more with it with more time, and it would be interesting to get deeper into it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
